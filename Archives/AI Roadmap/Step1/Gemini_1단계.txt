안정적인 금융 데이터 수집을 위한 핸들러 개발: 아키텍처 설계 및 구현 계획
본 문서는 yfinance와 pykrx 라이브러리를 활용하여 한국 주식 시장의 시세 데이터를 안정적으로 수집하는 데이터 핸들러 개발을 위한 종합적인 아키텍처 설계 및 단계별 구현 계획을 제시합니다. 이 계획은 단순한 스크립트 작성을 넘어, 유지보수성, 확장성, 재사용성을 갖춘 프로덕션 수준의 소프트웨어 컴포넌트 구축을 목표로 합니다. 이를 위해 모범적인 프로젝트 구조, 지능형 캐싱, 운영 관찰을 위한 로깅, 데이터 무결성 검증 프레임워크를 포함한 핵심 요소를 상세히 다룹니다.

I. 복원력 있는 데이터 핸들러를 위한 아키텍처 청사진
견고한 아키텍처는 프로젝트의 성공을 좌우하는 핵심 요소입니다. 초기 단계에서 명확성, 관심사의 분리, 현대적인 Python 패키징 표준 준수를 우선순위로 두어 유지보수와 확장이 용이한 시스템의 기틀을 마련해야 합니다.

1.1. 프로젝트 구조화: src 레이아웃 채택
프로젝트 구조는 개발 워크플로우와 최종 결과물의 품질에 직접적인 영향을 미칩니다. "플랫(flat)" 레이아웃과 "src" 레이아웃 중 하나를 선택하는 것은 중요한 첫 번째 아키텍처 결정입니다. 과거의 일부 가이드에서는 플랫 레이아웃을 권장하기도 했으나 , Python Packaging Authority (PyPA)를 포함한 현대적인 Python 패키징 표준은    

src 레이아웃을 강력히 선호합니다.   

src 레이아웃의 핵심적인 장점은 다음과 같습니다.

의도치 않은 임포트 방지: Python의 임포트 시스템은 현재 작업 디렉토리를 sys.path에 포함시키므로, 프로젝트 루트에서 개발 중인 소스 코드를 실수로 임포하는 상황이 발생할 수 있습니다. 이는 설치된 패키지 대신 로컬 소스 파일을 대상으로 테스트를 실행하게 만들어 패키징 오류를 감출 수 있는 심각한 문제를 야기합니다. src 레이아웃은 이러한 가능성을 원천적으로 차단합니다.   

설치 가능성 강제: src 레이아웃은 개발 및 테스트를 위해 pip install -e. 명령어를 통해 패키지를 편집 가능 모드로 설치하도록 강제합니다. 이 워크플로우는 프로젝트가 항상 설치 가능한 패키지로 취급되도록 보장하며, 테스트되는 대상과 배포되는 대상이 동일함을 보증합니다.

명확성과 조직화: 임포트 가능한 패키지 코드(src/)와 pyproject.toml, README.md, tests/와 같은 프로젝트 수준의 파일을 명확하게 분리하여 프로젝트 구조를 깔끔하게 유지합니다.   

이러한 구조적 선택은 단순히 디렉토리 이름을 정하는 것을 넘어, 개발자에게 더 규율 있고 견고한 개발 워크플로우를 강제하는 "강제 함수(forcing function)" 역할을 합니다. 이는 "스크립트 작성"에서 "패키지 구축"으로의 사고방식 전환을 유도하며, 재사용 가능하고 안정적인 핸들러를 만들려는 사용자의 목표와 정확히 일치합니다. 결과적으로 "내 컴퓨터에서는 잘 동작하는데"와 같은 일반적인 문제를 예방하는 효과를 가져옵니다.

아래 표는 본 프로젝트에서 채택할 구체적인 디렉토리 구조를 상세히 기술합니다. 이 구조는 모듈성을 증진하고 데이터 집약적인 Python 애플리케이션의 모범 사례에 부합합니다.   

표 1: 제안된 프로젝트 디렉토리 구조

경로	유형	목적
data_handler/	디렉토리	프로젝트 루트
├── data/	디렉토리	수집 및 검증된 데이터(예: Parquet 파일)의 기본 출력 위치. 버전 관리에서 제외됨.
├── logs/	디렉토리	순환 로그 파일 저장 위치. 버전 관리에서 제외됨.
├── src/	디렉토리	
애플리케이션 패키지의 기본 소스 코드를 포함.   

│ └── stock_handler/	디렉토리	
Python 패키지 자체. 이름은 짧고, 소문자이며, 설명적이어야 함.   

│ ├── __init__.py	파일	stock_handler를 Python 패키지로 만듦. 공개 API를 노출하는 데 사용 가능.
│ ├── main.py	파일	주 애플리케이션 로직 및 CLI 진입점을 정의.
│ ├── config.py	파일	중앙 집중식 설정(예: 기본 경로, API 설정, 검증 임계값).
│ ├── data_fetcher.py	파일	yfinance 및 pykrx와의 모든 상호작용을 담당하는 모듈.
│ ├── caching.py	파일	캐싱 로직(디스크 저장 및 로드)을 구현.
│ ├── validation.py	파일	모든 데이터 품질 및 무결성 검사를 포함.
│ └── utils.py	파일	로깅 설정과 같은 헬퍼 함수를 포함.
├── tests/	디렉토리	
모든 단위 및 통합 테스트를 포함. 소스 코드와 분리하여 관리.   

│ ├── test_data_fetcher.py	파일	데이터 수집 모듈에 대한 테스트.
│ └── test_validation.py	파일	데이터 검증 모듈에 대한 테스트.
├── target_tickers.csv	파일	
추적할 주식 종목 유니버스를 정의하는 사용자 제공 입력 파일.   

├── pyproject.toml	파일	
프로젝트 메타데이터 및 의존성을 명시하는 현대적인 표준.   

└── README.md	파일	
설정 및 사용법을 포함한 프로젝트 문서.   

1.2. 의존성 관리 및 환경
재현성을 보장하고 충돌을 방지하기 위해, 프로젝트의 의존성은 명시적으로 선언되고 가상 환경 내에서 격리되어야 합니다.   

pyproject.toml은 이러한 목적을 위한 현대적이고 통합된 표준으로, 많은 경우에 setup.py나 requirements.txt와 같은 오래된 파일들을 대체합니다. venv나 conda를 사용하여 가상 환경을 생성하고, pyproject.toml 파일에 pandas, yfinance, pykrx, pyarrow와 같은 모든 의존성을 정의하여 관리합니다.

1.3. 임포트 전략: 절대 경로 임포트
PEP 8은 절대 경로 임포트를 강력히 권장합니다. 절대 경로 임포트는 상대 경로 임포트보다 가독성이 높고 모호함이 적습니다. 예를 들어,    

from stock_handler.utils import setup_logger는 그 출처가 즉시 명확하지만, from.utils import setup_logger는 현재 파일의 패키지 내 위치를 알아야만 이해할 수 있습니다. 상대 경로 임포트는 자체 포함된 하위 패키지를 리팩토링할 때 유용할 수 있지만, 본 프로젝트의 구조는 비교적 단순하므로 명확성과 유지보수성 측면에서 절대 경로 임포트가 월등한 선택입니다. 따라서 모든 패키지 내부 임포트에 대해 절대 경로 임포트를 사용하는 정책을 적용하며,    

ruff나 isort와 같은 린터를 설정하여 이를 강제할 수 있습니다.

II. 핵심 데이터 수집 엔진
이 모듈은 애플리케이션의 심장부입니다. 서로 다른 API를 가진 데이터 소스를 처리할 수 있어야 하며, 네트워크 장애나 API 변경에 대해 복원력을 갖추어야 합니다.

2.1. 통합된 Fetcher 인터페이스
두 데이터 소스인 yfinance와 pykrx는 서로 다른 함수 시그니처와 관례를 가집니다. "어댑터(Adapter)" 또는 "퍼사드(Facade)" 패턴을 적용한 통합 인터페이스는 이러한 차이점을 주 애플리케이션 로직으로부터 추상화합니다. 이는 핵심 로직을 더 깔끔하게 만들고 향후 새로운 데이터 소스를 추가하는 작업을 단순화합니다.   

구현은 data_fetcher.py에 fetch_data(ticker, start_date, end_date)와 같은 주 함수를 생성하는 것으로 시작합니다. 이 함수는 주어진 티커에 적합한 소스를 결정하는 로직을 포함합니다. target_tickers.csv의 모든 티커는 한국 주식이므로 ,    

pykrx를 주 소스로 사용하고 yfinance를 대체 소스로 활용할 수 있습니다. CSV 파일의 티커는 각 라이브러리에 맞게 포맷팅되어야 합니다 (예: pykrx는 005930, yfinance는 005930.KS).

2.2. API별 어댑터
통합 인터페이스를 지원하기 위해 각 라이브러리의 특정 API 호출을 감싸는 래퍼 함수가 필요합니다. 이 래퍼들은 파라미터 매핑, 데이터 변환, 초기 오류 처리를 담당합니다.

_fetch_from_pykrx(ticker, start, end): 이 래퍼는 stock.get_market_ohlcv(start_date_str, end_date_str, ticker)를 호출합니다.   

datetime 객체를 pykrx가 요구하는 'YYYYMMDD' 형식의 문자열로 변환하는 역할을 합니다.

_fetch_from_yfinance(ticker, start, end): 이 래퍼는 yf.download(tickers=ticker_list, start=start_date, end=end_date, auto_adjust=True)를 호출합니다. 티커에 올바른 시장 접미사(   

.KS 또는 .KQ)를 추가하고, 결과로 반환되는 멀티 인덱스 데이터프레임을 pykrx의 출력과 일치하도록 표준화합니다.

2.3. 오류 처리 및 재시도 로직
외부 API에 대한 네트워크 요청은 본질적으로 신뢰할 수 없습니다. 핸들러는 일시적인 오류(예: 네트워크 타임아웃, 일시적인 API 비가용성)를 시스템 충돌 없이 우아하게 처리해야 합니다. 지수 백오프(exponential backoff)를 적용한 간단한 재시도 메커니즘은 표준적이고 효과적인 전략입니다.

API별 어댑터는 재시도 데코레이터로 장식됩니다. 이 데코레이터는 특정 예외(예: requests.exceptions.ConnectionError, HTTPError)를 포착하고, 점진적으로 길어지는 지연 시간 후에 호출을 재시도합니다. 정해진 횟수의 재시도 후에도 호출이 실패하면, 치명적인 오류를 로깅하고 None을 반환하여 주 루프가 다른 티커 처리를 계속할 수 있도록 합니다.

이러한 설계는 라이브러리의 불안정성을 고려할 때 필수적입니다. pykrx의 커밋 기록은 KRX 웹사이트 변경에 대응하기 위한 수정 사항("Added Referer header to requests to accommodate KRX website revamp")을 보여주며 ,    

yfinance 문서는 야후가 공식적으로 보증하지 않는 공개 API를 사용한다고 명시합니다. 두 라이브러리 모두 예고 없이 변경될 수 있는 비공식 API에 의존하므로, 오류 처리는 단순히 예외를 잡는 반응적인 수준을 넘어, 아키텍처적으로 선제적이어야 합니다. 특정 티커의 데이터 수집 실패가 전체 프로세스를 중단시켜서는 안 되며, 실패는 개별적으로 격리되어야 합니다. 이는 재시도 로직을 단순한 유틸리티가 아닌 데이터 수집기의 핵심 아키텍처 원칙으로 격상시키며,    

config.py에서 재시도 횟수와 백오프 계수를 설정 가능하게 만드는 등 더 견고한 구현을 정당화합니다.

III. 프로덕션 수준의 로깅 시스템 구현
로깅은 시스템의 관찰 가능성, 디버깅, 감사 추적에 매우 중요합니다. 잘 설계된 로깅 시스템은 능동적인 디버깅 없이도 애플리케이션의 동작에 대한 깊은 통찰력을 제공합니다.   

3.1. 중앙 집중식 로깅 설정
로깅을 한 곳에서 설정하면 애플리케이션 전체에 걸쳐 일관성을 보장할 수 있습니다. 이는 흩어져 있는 설정 코드를 방지하고 로깅 동작을 전역적으로 쉽게 변경할 수 있게 합니다. utils.py에 setup_logger() 함수를 생성하고, 이 함수를 애플리케이션의 진입점인 main.py에서 한 번만 호출하여 루트 로거를 설정합니다.

3.2. 다중 목적지 핸들러
서로 다른 환경과 사용 사례는 각기 다른 로깅 출력을 요구합니다. 실시간 모니터링은 간결한 콘솔 출력이 유리하며, 사후 디버깅은 상세하고 영구적인 파일 로그가 필요합니다.   

setup_logger() 함수는 두 개의 핸들러를 설정합니다.

logging.StreamHandler: 로그를 콘솔(sys.stdout)로 출력합니다. 이 핸들러의 기본 레벨은 INFO로 설정하여 사용자에게 고수준의 상태 업데이트를 제공합니다.

logging.handlers.RotatingFileHandler: 로그를 logs/ 디렉토리의 파일에 기록합니다. 이 핸들러의 레벨은 DEBUG로 설정하여 심층 분석을 위한 모든 세부 정보를 캡처합니다. maxBytes와 backupCount를 설정하여 로그 파일 크기를 자동으로 관리하고 디스크 공간 고갈을 방지합니다.   

3.3. 문맥적 포맷팅
파일 로그에 필요한 정보(타임스탬프, 모듈 이름, 줄 번호)는 콘솔에서 보기에는 너무 장황할 수 있습니다. 각 사용 사례에 최적화된 별도의 포맷터를 사용합니다.   

콘솔 포맷터: logging.Formatter('%(levelname)s: %(message)s') - 단순하고 깔끔한 형식.

파일 포맷터: logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') - 디버깅을 위한 풍부한 문맥을 제공.   

각 핸들러에는 해당 포맷터가 할당됩니다.

3.4. 모듈 수준 로깅
각 모듈에서 logging.getLogger(__name__)을 사용하면 패키지 구조를 반영하는 로거 계층이 생성됩니다. 이는 로그 상세 수준을 세밀하게 제어할 수 있게 하고 로그 메시지의 출처를 쉽게 추적할 수 있도록 합니다. 모든    

.py 파일(data_fetcher.py, validation.py 등)은 import logging; logger = logging.getLogger(__name__)로 시작하며, 해당 파일 내의 모든 로깅 호출은 이 logger 객체를 사용합니다.

아래 표는 이 애플리케이션의 맥락에서 각 로깅 레벨이 어떻게 사용될 것인지에 대한 규약을 정의하여, 일관되고 의미 있는 로깅을 보장합니다.

표 2: 애플리케이션 로깅 레벨 정의

레벨	본 애플리케이션에서의 목적	예시
DEBUG	개발자를 위한 상세 정보. 캐시 히트/미스, API 요청 파라미터, 중간 데이터 형태.	DEBUG: Cache miss for 005930.KS. Fetching from yfinance.
INFO	고수준의 진행 상황 및 상태 업데이트. 프로세스 시작/종료, 처리된 티커 수, 데이터 저장.	INFO: Successfully fetched and validated data for 35 of 40 tickers.
WARNING	문제가 발생했지만 애플리케이션은 계속 진행 가능. 데이터 검증 실패, 단일 티커 수집 불가.	WARNING: Ticker 000660.KS failed validation: 15 consecutive NaNs found.
ERROR	프로세스의 주요 부분을 완료하지 못하게 하는 심각한 오류. 입력 파일 읽기 불가, 캐시 디렉토리 쓰기 불가.	ERROR: Could not read ticker file from path: /nonexistent/path/tickers.csv
CRITICAL	애플리케이션을 강제 종료시키는 복구 불가능한 오류. (직접 사용은 드묾).	CRITICAL: Unhandled exception during application startup.

Sheets로 내보내기
IV. 지능형 캐싱 레이어 개발
속도가 느리거나 사용량이 제한된 외부 API에 반복적으로 접근하는 시스템에서 캐싱은 필수 기능입니다. 캐싱은 개발, 테스트, 반복 분석의 속도를 극적으로 향상시키면서 데이터 제공자에 대한 부하를 줄여줍니다.

4.1. 캐싱 전략 및 포맷
단순한 파일 기반 캐싱 메커니즘이 필요하며, 파일 포맷 선택은 성능에 중요합니다. CSV는 사람이 읽기 쉽지만, Parquet는 컬럼 기반 포맷으로, 특히 pandas 데이터프레임에 대해 훨씬 빠른 I/O와 효율적인 저장 공간을 제공합니다. caching.py 모듈은 pandas.DataFrame.to_parquet()와 pandas.read_parquet()를 사용하는 save_to_cache 및 load_from_cache 함수를 포함할 것입니다.

4.2. 캐시 키 생성
캐시는 서로 다른 티커, 날짜 범위, 데이터 소스에 대한 요청을 구별할 수 있어야 합니다. 이를 위해 견고하고 예측 가능한 파일 이름 규칙이 필수적입니다. 헬퍼 함수를 통해 f"{ticker}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}.parquet"와 같이 입력값에 기반한 캐시 파일 이름을 생성하여, 각 데이터 요청에 대해 고유하고 자체 설명적인 키를 만듭니다.

4.3. 캐시 무효화 및 관리
사용자는 최신 데이터를 가져오기 위해 캐시를 우회할 수 있어야 합니다. 이를 위한 간단한 메커니즘은 커맨드 라인 플래그를 사용하는 것입니다. 주 데이터 수집 로직은 먼저 --no-cache 플래그가 있는지 확인합니다. 플래그가 없으면 load_from_cache를 호출하여 캐시 히트를 시도합니다. 파일이 없거나 플래그가 있으면 캐시 미스로 간주하고 API에서 데이터를 가져옵니다. 데이터 수집 및 검증이 성공적으로 완료되면, 데이터는 save_to_cache를 통해 캐시에 저장됩니다.

V. 데이터 무결성 보장 프레임워크
어떤 소스에서든 원시 데이터는 신뢰할 수 없습니다. 검증 파이프라인은 데이터가 분석이나 의사결정에 사용되기 전에 그 신뢰성을 보장하는 데 매우 중요합니다. 이는 프로젝트를 단순한 "다운로더"에서 진정한 "데이터 핸들러"로 격상시킵니다.

5.1. 데이터 검증 파이프라인
데이터 품질 검사는 모듈화되어 순차적으로 적용되어야 합니다. 데이터프레임은 이전 검사를 통과한 경우에만 다음 검사로 전달되어야 명확하고 디버깅 가능한 흐름이 만들어집니다. validation.py의 주 validate_dataframe(df, start_date, end_date) 함수는 일련의 비공개 검증 함수들을 호출하고, 불리언 상태와 검증 오류 목록을 반환합니다.

5.2. 스키마 및 유형 검증
가장 기본적인 첫 번째 검사는 데이터가 예상된 구조를 가지고 있는지 확인하는 것입니다. 이는 예기치 않은 API 변경이나 파싱 오류를 잡아냅니다. 입력 파일인 target_tickers.csv 자체는 구분,순위,종목명,Ticker 열을 가지고 있으며 , 다운로드된 데이터는 표준 OHLCV 열을 가져야 합니다. 이 검사는 데이터프레임이 예상된 열(예:    

['Open', 'High', 'Low', 'Close', 'Volume'])을 포함하고 인덱스가 pandas.DatetimeIndex인지 확인합니다.

5.3. 시간적 연속성 검사
시계열 분석에서는 거래일 누락과 같은 데이터 공백이 있는지 아는 것이 중요합니다. 데이터는 예기치 않은 중단 없이 요청된 전체 기간을 포괄해야 합니다. 이 검사는    

pandas.date_range(start=start_date, end=end_date, freq='B') ('B'는 영업일)를 사용하여 예상 날짜 범위를 생성하고 , 이를 데이터프레임의 실제 인덱스와 비교합니다. 누락된 날짜가 있으면 경고를 발생시킵니다.   

5.4. 결측 데이터(NaN) 분석
결측값(NaN)은 많은 금융 계산을 무효화할 수 있습니다. NaN의 존재 여부뿐만 아니라, 그 양을 정량화해야 합니다. 특히, 길고 연속적인 NaN 블록은 흩어져 있는 단일 NaN들보다 훨씬 더 해롭습니다.   

시계열 데이터 품질에 대한 이러한 미묘한 이해는 전문가 수준 시스템의 특징입니다. 결측 데이터의 패턴은 단순한 양보다 데이터 품질의 더 중요한 지표입니다. 따라서 검증 프레임워크는 두 가지 사례를 별개의 검사로 취급하고 서로 다른 실패 임계값을 적용해야 합니다.

두 가지 별개의 검사가 구현될 것입니다:

총 NaN 개수: df.isnull().sum().sum()을 계산합니다. NaN 값의 비율이 설정 가능한 임계값(예: 전체 데이터 포인트의 5%)을 초과하면 데이터를 부적합으로 표시합니다.

연속 NaN 개수: 이것이 더 중요한 검사입니다. groupby().cumsum() 패턴을 사용하여 'Close' 가격 열에서 연속적인 NaN 블록의 길이를 찾습니다. 가장 긴 블록의 길이가 임계값(예: 5일)을 초과하면 데이터는 신뢰할 수 없는 것으로 간주되어 검증에 실패합니다.   

표 3: 데이터 검증 검사 요약

검사 이름	목적	구현 방법	기본 임계값 (설정 가능)
스키마 검사	데이터프레임이 필수 열과 DatetimeIndex를 가졌는지 확인.	df.columns 및 isinstance(df.index, pd.DatetimeIndex) 확인.	N/A (하드코딩된 스키마)
시간적 커버리지	요청된 범위 내에 누락된 영업일이 없는지 확인.	df.index를 pd.date_range(start, end, freq='B')와 비교.	> 0일 누락 (경고)
총 NaN 비율	전체 결측값 비율이 낮은지 확인.	df.isnull().sum().sum() / df.size	> 5% (실패)
최대 연속 NaN	길고 중단 없는 결측 데이터 기간이 없는지 확인.	df['Close'].isnull()에 groupby().cumsum() 패턴을 사용하여 최대 블록 크기 찾기.	> 5일 (실패)

Sheets로 내보내기
VI. 유연한 커맨드 라인 인터페이스(CLI) 구축
CLI는 데이터 핸들러를 단순한 라이브러리가 아닌 사용 가능한 도구로 만듭니다. 이를 통해 사용자는 소스 코드를 수정하지 않고도 다양한 파라미터로 데이터 수집 프로세스를 실행할 수 있습니다.

6.1. argparse를 이용한 인자 파싱
Python의 내장 argparse 모듈은 견고하고 자체 문서화되는 CLI를 만드는 표준적인 방법입니다. 도움말 메시지, 인자 유형, 오류 보고를 자동으로 처리합니다.    

main.py에서 ArgumentParser를 생성하고 애플리케이션의 모든 주요 설정 가능한 측면에 대한 인자를 정의합니다.

6.2. 진입점 스크립트 (main.py)
이 스크립트는 전체 워크플로우를 조율하며, CLI 인자를 파싱하고 다른 모듈의 적절한 함수를 올바른 순서로 호출합니다. 주 실행 블록은 다음 단계를 따릅니다.

CLI 인자 파싱.

지정된 로그 레벨로 utils.setup_logger() 호출.

지정된 CSV 파일에서 티커 목록 로드.

각 티커에 대해 반복:
a. 티커 처리 시작 로깅.
b. 날짜 및 캐시 플래그를 전달하여 data_fetcher.fetch_data() 호출.
c. 데이터가 반환되면 validation.validate_dataframe() 호출.
d. 검증을 통과하면 최종 데이터를 data/ 디렉토리에 저장.
e. 티커에 대한 결과(성공, 검증 실패, 수집 실패) 로깅.

최종 요약 보고서 로깅.

표 4: CLI 인자 명세

인자	단축 플래그	유형	필수	기본값	도움말 텍스트
--start-date	-s	str	예	N/A	데이터 수집 시작일 (YYYY-MM-DD 형식).
--end-date	-e	str	예	N/A	데이터 수집 종료일 (YYYY-MM-DD 형식).
--ticker-file	-t	str	아니요	target_tickers.csv	티커 목록이 포함된 CSV 파일 경로.
--output-dir	-o	str	아니요	data/	최종 Parquet 파일을 저장할 디렉토리.
--no-cache	-nc	flag	아니요	False	설정 시 캐시를 우회하고 API에서 새로운 데이터를 가져옴.
--log-level	-l	str	아니요	INFO	콘솔 로깅 레벨 설정 (DEBUG, INFO, WARNING, ERROR).

Sheets로 내보내기
VII. 반복적 개발 로드맵
반복적이고 점진적인 방법론을 따르면 신속한 프로토타이핑, 지속적인 피드백, 관리 가능한 개발 주기가 가능해집니다. 각 스프린트는 최종 제품의 실질적이고 작동하는 하위 집합을 제공합니다.

스프린트 1: 기초 핵심 기능 (The "Tracer Bullet")
목표: 프로젝트 골격을 확립하고 핵심 데이터 수집 기능의 작동을 증명합니다.

과업:

src 레이아웃, pyproject.toml을 포함한 전체 프로젝트 구조 설정.

data_fetcher.py에 pykrx와 yfinance 양쪽에서 하드코딩된 단일 티커에 대한 기본 데이터 수집 기능 구현.

utils.py에 간단한 콘솔 전용 로거 구현.

이들을 연결하는 초기 main.py 생성.

결과: 하나의 주식에 대한 데이터를 가져와 기본 로깅과 함께 화면에 출력할 수 있는 실행 가능한 스크립트.

스프린트 2: 견고성 및 성능 강화
목표: 반복 실행에 대해 핸들러를 복원력 있고 효율적으로 만듭니다.

과업:

파일 순환 기능이 있는 완전한 이중 핸들러 로깅 시스템 구현.

caching.py에 파일 기반 Parquet 캐싱 레이어 구현.

데이터 수집기에 캐싱 로직 통합.

API 래퍼에 견고한 오류 처리 및 재시도 로직 추가.

결과: 티커 목록을 처리하고, 후속 실행 속도를 높이기 위해 결과를 캐시하며, 상세 정보를 콘솔과 관리되는 로그 파일 양쪽에 기록하는 핸들러.

스프린트 3: 품질 보증 및 사용성 확보
목표: 데이터 품질 게이트와 사용자 친화적인 인터페이스를 추가하여 애플리케이션을 완성합니다.

과업:

표 3에 명시된 대로 validation.py에 완전한 데이터 검증 파이프라인 구현.

main.py의 주 워크플로우에 검증 단계 통합.

표 4에 명시된 대로 완전한 argparse CLI 구축.

설정 및 사용 지침이 포함된 README.md 작성.

결과: 커맨드 라인에서 실행하여 고품질 금융 데이터를 안정적으로 수집, 검증, 저장할 수 있는 완전하고 설정 가능한 견고한 데이터 핸들러.